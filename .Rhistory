sig.year <- rnorm(nyrs, 0, sig.sig.year)
# TEMPERATURE COVARIATE
bTemp.sig <- 0.5
temp <- matrix(rnorm(max.sites*nyrs, 50, 7), nrow = max.sites, ncol = nyrs)
#SCALED
temp_mean <- mean(temp)
temp_sd <- sd(temp)
temp_sc <- (temp - temp_mean) / temp_sd
#SIGMA
sigma <- exp(ob + bTemp.sig * temp_sc +
matrix(rep(sig.year, each = max.sites), nrow = max.sites, ncol = nyrs))
#####
# ----  Abundance component: Site effect, Year effect and Year trend
# Site effect (RANDOM INTERCEPT)
mu.lam.alpha.site <- log(1.5)
sig.lam.alpha.site <- 0.5
lam.alpha.site <- rnorm(max.sites, mu.lam.alpha.site, sig.lam.alpha.site)
# Year effect (RANDOM)
sig.lam.year <- 0.7
lam.year <- rnorm(nyrs, 0, sig.lam.year)
#TIME CO-VARIATE (YEAR)
b.lam.year <- 0.3
year <- matrix(NA,nrow = max.sites, ncol = nyrs)
colnames(year) <- yrs
for (i in 0:nyrs){
year[ ,yrs[i]] <- rep(year_number[i], max.sites)
}
# AUTOCORRELATION AND OVERDISPERSION TERM
rho <- 0.5 # Autoregressive parameter
sig.lam.eps <- 0.2
eps <- matrix(NA,nrow = max.sites, ncol = nyrs) # Unstructured random variation for overdispersion
for (j in 1:max.sites){
for (t in 1:nyrs){
eps[j,t] <- rnorm(1,0,sig.lam.eps)
}
}
w <- matrix(NA,nrow = max.sites, ncol = nyrs)
lam <- matrix(NA,nrow = max.sites, ncol = nyrs)
# First year
for(j in 1:max.sites){
w[j,1] <- eps[j,1] / sqrt(1 - rho * rho)
lam[j,1] <- exp(lam.alpha.site[j] +
lam.year[1] +
b.lam.year*year[j,1] +
w[j,1])
}
# Later years
for (j in 1:max.sites){
for (t in 2:nyrs){
w[j,t] <- rho * w[j,t-1] + eps[j,t]
lam[j,t] <- exp(lam.alpha.site[j] +
lam.year[t] +
b.lam.year*year[j,t] +
w[j,t])
}
}
lam.tot <- colSums(lam)
######
# ---- Generate ABUNDANCE per site and year ----
# Abundance
N <- matrix(NA,nrow = max.sites, ncol = nyrs)
for (j in 1:max.sites){
for (t in 1:nyrs){
N[j,t] <- rpois(1,lam[j,t])
}}
N.tot <- colSums(N)
# Introduce NA (not sampled)
vec <- seq(1,length(N))
na <- sample(vec, 100)
N[na]<-NA
# Cluster size (to correct) per site and year
clus <- list()
for (t in 1:nyrs){
clus[[t]] <- rpois(nSites[t], 1.5)
}
clusLong <- ldply(clus,cbind) # 1 long vector with all abundances per site and year
clus3 <- ldply(clus,rbind)
clus_size <- t(clus3) # CLUSTER SIZE per site and year stored in a matrix with columns (this is not really necessary to do, with inventing an average would be fine)
average_clus <- mean(clus_size, na.rm = TRUE)
# Correct N with average cluster size
Nclus <- N * average_clus
Nclus.tot <- colSums(Nclus,na.rm = TRUE) # Total pop.abundance corrected by cluster size
####
# ---- Simulate continuous distance data ----
# Nc = count of individuals detected in each distance interval
yList <- list()
for (i in 1:nyrs){
yList[[i]] <- array(0, c(nSites[i], length(dist.breaks)-1))
}
for (t in 1:nyrs){
for(j in 1:max.sites) {
if(N[j,t] == 0 | is.na(N[j,t]))
next
# Distance from observer to the individual
d <- runif(N[j,t], 0, strip.width) 		# Uniform distribution of animals
# Simulates one distance for each individual in the site (N[j])
p <- g(x=d, sig=sigma[j,t])   		# Detection probability. Sigma is site-time specific
seen <- rbinom(N[j,t], 1, p)
if(all(seen == 0))
next
d1 <- d[seen==1] 				# The distance data for seen individuals
counts <- table(cut(d1, dist.breaks, include.lowest=TRUE))
yList[[t]][j,] <- counts 				# The number of detections in each distance interval
}}
y.sum.sites <- lapply(yList, function(x) rowSums(x)) # Total count per site each year
y.sum.sites2 <- ldply(y.sum.sites,rbind)
y.sum <- t(y.sum.sites2) # y per site and year stored in a matrix with columns
y.sum[na] <- NA # Add what are real NA generated from Na in N (not 0)
####
# ---- Convert data to JAGS format ----
nind.year <- lapply(yList,sum)
nind <- sum(unlist(nind.year, use.names = F))
y.sum # Matrix with counts
# Co-variates
temp_sc
ob.id # Matrix with observers
year_number # Vector with year variable
site <- c(1:max.sites)
year <- c(1:nyrs)
# Get one long vector with years, distance category and site
#site <- dclass <- year <- NULL
dclass <- site.dclass <- year.dclass <- NULL # Fixed index to map dclass onto site and year
for (t in 1:nyrs){
for(j in 1:max.sites){
if (y.sum[j,t] == 0 | is.na(y.sum[j,t]))
next
#site <- c(site, rep(j, y.sum[j,t])) # site index: repeat the site as many times as counts in that site (for multi model??)
# vector of sites through years (disregarding distance class)
#year <- c(year, rep(t, y.sum[j,t]))
for (k in 1:nG){
if (yList[[t]][j,k] == 0) # Refers for the ditance classes to the list with years and bins
next
dclass <- c(dclass, rep(k, yList[[t]][j,k]))	# Distance category index
site.dclass <- c(site.dclass, rep(j, yList[[t]][j,k]))
year.dclass <- c(year.dclass, rep(t, yList[[t]][j,k]))
}}
}
# Create one matrix for indexing year when calculating abundance per year in JAGS
allyears <- NULL
for (i in 1:nyrs){
allyears <- c(allyears,rep(yrs[i],nSites[i]))
}
m <- data.frame(allyears = allyears)
m$allyears <- as.factor(m$allyears)
indexYears <- model.matrix(~ allyears-1, data = m)
####
# ---- Compile data for JAGS model ----
data1 <- list(nyears = nyrs, nsites = max.sites, nG=nG, int.w=int.w, strip.width = strip.width, midpt = midpt, db = dist.breaks,
year.dclass = year.dclass, site.dclass = site.dclass, y = y.sum, nind=nind, dclass=dclass,
tempCov = temp_sc, ob = ob.id, nobs = nobs, year1 = year_number, site = site, year_index = yrs)
# ---- JAGS model ----
setwd("S:/PhD/Second chapter/Data/Model")
cat("model{
# PRIORS
# PRIORS FOR LAMBDA
rho ~ dunif(-1,1) # Autorregresive parameter (serial AC)
tau <- pow(sd, -2) # Prior for overdispersion in eps
sd ~ dunif(0, 3)
bYear.lam ~ dnorm(0, 0.001) # Prior for the trend
# Random effects for lambda per site
mu.lam.site ~ dunif(-10, 10)
sig.lam.site ~ dunif(0, 10)
tau.lam.site <- 1/(sig.lam.site*sig.lam.site)
for (j in 1:nsites){
log.lambda.site[j] ~ dnorm(mu.lam.site, tau.lam.site)
}
# Random effects for lambda per year
sig.lam.year ~ dunif(0, 10)
tau.lam.year <- 1/(sig.lam.year*sig.lam.year)
log.lambda.year[1] <- 0
for (t in 2:nyears){
log.lambda.year[t] ~ dnorm(0, tau.lam.year)
}
# PRIORS FOR SIGMA
bTemp.sig ~ dnorm(0, 0.001)
mu.sig ~ dunif(-10, 10) # Random effects for sigma per observer
sig.sig ~ dunif(0, 10)
tau.sig <- 1/(sig.sig*sig.sig)
# Random observer effect for sigma
for (o in 1:nobs){
sig.obs[o] ~ dnorm(mu.sig, tau.sig)
}
# Random effects for sigma per year
sig.sig.year ~ dunif(0, 10)
tau.sig.year <- 1/(sig.sig.year*sig.sig.year)
for (t in 1:nyears){
log.sigma.year[t] ~ dnorm(0, tau.sig.year)
}
for(i in 1:nind){
dclass[i] ~ dcat(fct[site.dclass[i], year.dclass[i], 1:nG])
# Bayesian p-value for detection component (Bp.Obs)
dclassnew[i] ~ dcat(fct[site.dclass[i], year.dclass[i], 1:nG]) # generate new observations
Tobsp[i]<- pow(1- sqrt(fct[site.dclass[i], year.dclass[i],dclass[i]]),2) # Test for observed data
Tobspnew[i]<- pow(1- sqrt(fct[site.dclass[i], year.dclass[i],dclassnew[i]]),2) # Test for new data
}
Bp.Obs <- sum(Tobspnew[1:nind]) > sum(Tobsp[1:nind])
# LIKELIHOOD
# FIRST YEAR
for(j in 1:nsites){
sigma[j,1] <- exp(sig.obs[ob[j,1]] + bTemp.sig*tempCov[j,1] + log.sigma.year[year_index[1]])
# Construct cell probabilities for nG multinomial cells (distance categories) PER SITE
for(k in 1:nG){
up[j,1,k]<-pnorm(db[k+1], 0, 1/sigma[j,1]^2) ##db are distance bin limits
low[j,1,k]<-pnorm(db[k], 0, 1/sigma[j,1]^2)
p[j,1,k]<- 2 * (up[j,1,k] - low[j,1,k])
pi[j,1,k] <- int.w[k] / strip.width
f[j,1,k]<- p[j,1,k]/f.0[j,1]/int.w[k]                   ## detection prob. in distance category k
fc[j,1,k]<- f[j,1,k] * pi[j,1,k]                 ## pi=percent area of k; drops out if constant
fct[j,1,k]<-fc[j,1,k]/pcap[j,1]
}
pcap[j,1] <- sum(fc[j,1, 1:nG]) # Different per site and year (sum over all bins)
f.0[j,1] <- 2 * dnorm(0,0, 1/sigma[j,1]^2) # Prob density at 0
y[j,1] ~ dbin(pcap[j,1], N[j,1])
N[j,1] ~ dpois(lambda[j,1])
lambda[j,1] <- exp(log.lambda.site[site[j]] + log.lambda.year[year_index[1]] + bYear.lam*year1[1] + w[j,1]) # year1 is t-1; year_index is t (to index properly the random effect)
w[j,1] <- eps[j,1] / sqrt(1 - rho * rho)
eps[j,1] ~ dnorm(0, tau)
# Bayesian p-value on abundance component
Nnew[j,1]~dpois(lambda[j,1]) ##Create replicate abundances for year 1
FT1[j,1]<-pow(sqrt(N[j,1])-sqrt(lambda[j,1]),2) ### residuals for 'observed' and new abundances in year 1
FT1new[j,1]<-pow(sqrt(Nnew[j,1])-sqrt(lambda[j,1]),2)
}
#############
# LATER YEARS
for(j in 1:nsites){
for (t in 2:nyears){
sigma[j,t] <- exp(sig.obs[ob[j,t]] + bTemp.sig*tempCov[j,t] + log.sigma.year[year_index[t]])
# Construct cell probabilities for nG multinomial cells (distance categories) PER SITE
for(k in 1:nG){
up[j,t,k]<-pnorm(db[k+1], 0, 1/sigma[j,t]^2) ##db are distance bin limits
low[j,t,k]<-pnorm(db[k], 0, 1/sigma[j,t]^2)
p[j,t,k]<- 2 * (up[j,t,k] - low[j,t,k])
pi[j,t,k] <- int.w[k] / strip.width
f[j,t,k]<- p[j,t,k]/f.0[j,t]/int.w[k]                   ## detection prob. in distance category k
fc[j,t,k]<- f[j,t,k] * pi[j,t,k]                 ## pi=percent area of k; drops out if constant
fct[j,t,k]<-fc[j,t,k]/pcap[j,t]
}
pcap[j,t] <- sum(fc[j,t, 1:nG]) # Different per site and year (sum over all bins)
f.0[j,t] <- 2 * dnorm(0,0, 1/sigma[j,t]^2) # Prob density at 0
y[j,t] ~ dbin(pcap[j,t], N[j,t])
N[j,t] ~ dpois(lambda[j,t])
lambda[j,t] <- exp(log.lambda.site[site[j]] + log.lambda.year[year_index[t]] + bYear.lam*year1[t] + w[j,t])
w[j,t] <- rho * w[j,t-1] + eps[j,t]
eps[j,t] ~ dnorm(0, tau)
# Bayesian p-value on abundance component (rest of years)
Nnew[j,t]~dpois(lambda[j,t]) # create replicate abundances for rest of the years
FT1[j,t]<-pow(sqrt(N[j,t])-sqrt(lambda[j,t]),2) # residuals for 'observed' and new abundances for the rest of the years
FT1new[j,t]<-pow(sqrt(Nnew[j,t])-sqrt(lambda[j,t]),2)
}
}
T1p <- sum(FT1[1:nsites,1:nyears]) #Sum of squared residuals for actual data set (RSS test)
T1newp <- sum(FT1new[1:nsites,1:nyears]) # Sum of squared residuals for new data set (RSS test)
# Bayesian p-value
Bp.N <- T1newp > T1p
# Derived parameters
for(t in 1:nyears){
popindex[t] <- sum(lambda[,t])
}
# Expected abundance per year inside model
lam.tot[1] <- popindex[1] # Expected abundance in year 1
for (i in 2:nyears){
lam.tot[i] <- lam.tot[i-1] * # Here I add the starting population size as a baseline for the trend
exp(bYear.lam)}
}",fill=TRUE, file = "s_sigma(integral)[obs(o,j,t)_covTemp(j,t)_year.random(t)]_lambda[alpha.site.random(j)_year.random(t)_beta.year(j)_w]_BayesP.txt")
# Inits
Nst <- y.sum + 1
inits <- function(){list(mu.sig = runif(1, log(30), log(50)), sig.sig = runif(1),
mu.lam.site = runif(1), sig.lam.site = 0.2, sig.lam.year = 0.3, bYear.lam = runif(1),
N = Nst)}
# Params
params <- c( "mu.sig", "sig.sig", "bTemp.sig", "sig.obs", "log.sigma.year", # Save also observer effect
"mu.lam.site", "sig.lam.site", "sig.lam.year", "bYear.lam", "log.lambda.year", # Save year effect
"popindex", "sd", "rho", "lam.tot",'Bp.Obs', 'Bp.N'
)
# MCMC settings
nc <- 3 ; ni <- 10000 ; nb <- 2000 ; nt <- 5
# With jagsUI
# With jagsUI
out <- jags(data1, inits, params, "s_sigma(integral)[obs(o,j,t)_covTemp(j,t)_year.random(t)]_lambda[alpha.site.random(j)_year.random(t)_beta.year(j)_w]_BayesP.txt", n.chain = nc,
n.thin = nt, n.iter = ni, n.burnin = nb, parallel = TRUE)
summary <- out$summary
setwd("S:/PhD/Second chapter/Data/Results/TRIM")
save(out, file = "6.TRIM.RData") # 60000 iter, 4 thining
load("6.TRIM.RData")
out$summary
data_comp <- list(lam.tot = lam.tot,
mu.sig.obs = mu.sig.obs, sig.sig.obs = sig.sig.obs,
bTemp.sig = bTemp.sig,
mu.lam.alpha.site = mu.lam.alpha.site,
sig.lam.alpha.site = sig.lam.alpha.site,
sig.lam.year = sig.lam.year,
b.lam.year = b.lam.year,
rho = rho, sig.lam.eps = sig.lam.eps)
y.sum
data_comp
summary <- out$summary
View(summary)
strip_width <- 200
sigma <- 30 # Scale parameter of half-normal detection function
g <- function(x, sig) exp(-x^2/(2*sig^2)) # Function definition
g(30, sig=sigma) # Example: Detection probability at a distance of 30m
par(mfrow=c(1,2))
curve(g(x, sig=30), 0, 200, xlab="Distance (x)", ylab="Detection prob.", lwd = 2, frame = F)
curve(g(x, sig=60), 0, 200, add=TRUE, lty = 2, lwd = 2)
# Sigma of 60 looks more general to represent my data
sigma <- 60
N = 300
sigma = 60
par(mfrow = c(1,2))
curve(exp(-x^2/(2*sigma^2)), 0, 200, xlab="Distance (x)", ylab="Detection prob.", lwd =
2, main = "Detection function", ylim = c(0,1))
text(80, 0.9, paste("sigma:", sigma))
xall <- runif(N, -200, 200) # Random values between -200 and +200 with uniform dist
# Uniform because it is true distances and we asume that
# density is the same in all area
hist(abs(xall), nclass=10, xlab = "Distance (x)", col = "grey", main = "True (grey) \nand
observed distances (blue)") # Plot dist true N in abs values and 10 clases for clarity
g <- function(x, sig) exp(-x^2/(2*sig^2)) # Define detection function (p = g(x,sig))
p <- g(xall, sig=sigma) # p at each true distance given sig
y <- rbinom(N, 1, p) # For each observation draw a bernouilli trial 1/0 (detect/nondetect)
hist(x, col = "blue", add = TRUE) # Plot on top of true N, observed n (distances)
# 3. Obtain detection probability (p) at each distance
g <- function(x, sig) exp(-x^2/(2*sig^2)) # Define detection function (p = g(x,sig))
p <- g(xall, sig=sigma) # p at each true distance given sig
# 4. Separate detected vS non detected (Simulate the n sample)
y <- rbinom(N, 1, p) # For each observation draw a bernouilli trial 1/0 (detect/nondetect)
# From the specific p for each individual (calculated from g(x,sig))
# 5. Select the distances of n (y = 1 or observed individuals)
x <- xall[y==1]
x <- abs(x) # now it doesn't have direction
hist(x, col = "blue", add = TRUE) # Plot on top of true N, observed n (distances)
library(dplyr)
########################################################
####                  Check convergence           #####
########################################################
rm(list=ls())
setwd("S:/PhD/Second chapter/Data")
d <- read.csv("DataDS_ready_ALL.csv")
colnames(d)[which(colnames(d) == "Count")] <- "Cluster"
library(rtrim)
library(dplyr)
setwd("S:/PhD/Second chapter/Data")
# Load species names
s <- read.csv("sp_trend_dg.csv", sep = ";")
s_good <- as.vector(s$Species[which(s$include_samplesize == 1)])
remove_6 <- c("CACHL", "CAINA", "CIJUN", "COCOT", "COLIV", "LUARB", "LUMEG", "MIMIG", "OEHIS", "ORORI", "PIVIR", "PYRAX", "STUNI", "STVUL", "TUMER", "TUVIS")
s_good <- s_good[-which(s_good %in% remove_6)]
setwd("S:/PhD/Second chapter/Data/Results/TRIM/6temp/Final")
species <- list()
at <- list()
for (xxx in 1:length(s_good)){
load(paste("HDS_",s_good[xxx],".RData", sep = ""))
# SPECIES
at[[1]] <- paste("HDS_",s_good[xxx], sep = "")
# JAGS SUMMARY
summary <- as.data.frame(out$summary)
at[[2]] <- summary
# CONVERGENCE
at[[3]] <- summary[which(summary$Rhat > 1.1), ]
# COUNTS PER YEAR
d_tr <- d[ ,which(colnames(d) %in% c("Species",  "T_Y", "Observer"))]
d_tr_all <- data.frame(T_Y = unique(d_tr$T_Y), id = NA)
d_tr$Observer <- as.character(d_tr$Observer)
d_tr_all_obs <- left_join(d_tr_all, d_tr)
d_tr_all_obs <- d_tr_all_obs[ ,c(1,4)]
d_tr_all_obs <- d_tr_all_obs[which(!duplicated(d_tr_all_obs)), ] # Table with all sampled fields and which observer sampled it
sp <- d[which(d$Species == s_good[xxx]), which(colnames(d) %in% c("Year", "Banda", "transectID", "T_Y", "Species", "Observer", "Cluster"))] # Select species spAL and all years
sp <- arrange(sp, Year, transectID) #Ordered
sp_detec_transectID <- unique(sp$transectID)
sp$Observer <- as.character(sp$Observer)
absent <- anti_join(d_tr_all,sp) # Transects with 0 abundance, add to sp.
colnames(absent)[2] <- "Banda" # Format it to add the rows to sp
absent$T_Y <- as.character(absent$T_Y)
absent$Species <- s_good[xxx]
absent$Cluster <- NA
absent <- left_join(absent, d_tr_all_obs)
for (i in 1:nrow(absent)){ # Format to join absent - detections
cent <- substr(absent$T_Y[i], 10,10) # To include SI102 (cents)
cent <- as.numeric(cent)
if(is.na(cent)){
absent$Year[i] <- substr(absent$T_Y[i], 6,9)
absent$transectID[i] <- substr(absent$T_Y[i], 1,4)
} else { absent$Year[i] <- substr(absent$T_Y[i], 7,10)
absent$transectID[i] <- substr(absent$T_Y[i], 1,5)}
}
absent$count <- 0
sp$count <- 1
all_sp <- rbind(sp,absent) # Include transects with abundance 0
all_sp <- arrange(all_sp, Year, transectID) # Ordered
absent$count <- 0
yrs <- c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018) # I HAVE TO CONVERT THIS FROM 0-7 (but nyrs is still 8!)
nyrs <- length(yrs)
all.sites <- unique(all_sp$transectID)
all.sites <- sort(all.sites,descreasing = TRUE)
max.sites <- length(all.sites)
m <- matrix(NA, nrow = length(all.sites), ncol = nyrs)
rownames(m) <- all.sites
colnames(m) <- yrs
count <- aggregate(Species ~ Year + transectID, FUN = length, data = sp)
for (i in 1:nrow(count)){ # Add counts > 0
m[which(rownames(m) %in% count$transectID[i]), which(colnames(m) %in% count$Year[i])] <- count$Species[i]}
for (i in 1:nrow(absent)){ # Add absences (0)
m[which(rownames(m) %in% absent$transectID[i]), which(colnames(m) %in% absent$Year[i])] <- absent$count[i]}
at[[4]] <- colSums(m,na.rm = TRUE)
species[[xxx]] <- at
print(xxx)
}
# s_good <- c("BUOED", "CIAER", "COMON", "COPAL", "FATIN", "MECAL", "PAMAJ")
which(s_good == "BUOED")
species[[2]][[3]]
# s_good <- c("BUOED", "CIAER", "COMON", "COPAL", "FATIN", "MECAL", "PAMAJ")
which(s_good == "CIAER")
species[[4]][[3]]
# s_good <- c("BUOED", "CIAER", "COMON", "COPAL", "FATIN", "MECAL", "PAMAJ")
which(s_good == "COMON")
species[[6]][[3]]
species[[7]][[3]]
# s_good <- c("BUOED", "CIAER", "COMON", "COPAL", "FATIN", "MECAL", "PAMAJ")
which(s_good == "COPAL")
species[[8]][[3]]
# s_good <- c("BUOED", "CIAER", "COMON", "COPAL", "FATIN", "MECAL", "PAMAJ")
which(s_good == "FATIN")
species[[9]][[3]]
# s_good <- c("BUOED", "CIAER", "COMON", "COPAL", "FATIN", "MECAL", "PAMAJ")
which(s_good == "MECAL")
species[[15]][[3]]
# s_good <- c("BUOED", "CIAER", "COMON", "COPAL", "FATIN", "MECAL", "PAMAJ")
which(s_good == "PAMAJ")
species[[18]][[3]]
setwd("S:/PhD/Second chapter/Data")
s <- read.csv("sp_trend_dg.csv", sep = ";")
s_good <- as.vector(s$Species[which(s$include_samplesize == 1)])
remove_6 <- c("CACHL", "CAINA", "CIJUN", "COCOT", "COLIV", "LUARB", "LUMEG", "MIMIG", "OEHIS", "ORORI", "PIVIR", "PYRAX", "STUNI", "STVUL", "TUMER", "TUVIS")
s_good <- s_good[-which(s_good %in% remove_6)] # SPECIES THAT CONVERGE FOR MODEL 6
setwd("S:/PhD/Second chapter/Data/Results/TRIM/6temp")
load("spConvergence_light.RData")
Bp6 <- data.frame(matrix(NA,ncol = 5,nrow = length(s_good)))
colnames(Bp6) <- c("sp", "Bp.Obs", "lci", "uci", "over_0")
for (i in 1:length(s_good)) {
sum <- data.frame(species[[i]][[2]])
est_Bp6 <- sum[which(rownames(sum) %in% "Bp.Obs"), c(1,3,7,10)]
Bp6[i,1] <- s_good[i]
Bp6[i,c(2:5)] <- est_Bp6
}
Bp6_bad <- Bp6[which(Bp6$Bp.Obs < 0.1 | Bp6$Bp.Obs > 0.9), ]
Bp6_bad
setwd("S:/PhD/Second chapter/Data/Results/TRIM/6temp/HR_df")
load("spConvergence_light.RData")
setwd("S:/PhD/Second chapter/Data/Results/TRIM/6temp/HR_df")
setwd("S:/PhD/Second chapter/Data/Results/TRIM/6temp/HR_df")
s_good <- c("CIAER", "COGAR", "COMON", "FATIN", "HIRUS", "LASEN", "MICAL", "PADOM", "PAMON", "PIPIC")
# LIGHT (WITH ONLY SUMMARY)
species <- list()
at <- list()
for (xxx in 1:length(s_good)){
load(paste("HDS_",s_good[xxx],".RData", sep = ""))
# SPECIES
at[[1]] <- paste("HDS_",s_good[xxx], sep = "")
# JAGS SUMMARY
summary <- as.data.frame(out$summary)
at[[2]] <- summary
# CONVERGENCE
at[[3]] <- summary[which(summary$Rhat > 1.1), ]
# COUNTS PER YEAR
d_tr <- d[ ,which(colnames(d) %in% c("Species",  "T_Y", "Observer"))]
d_tr_all <- data.frame(T_Y = unique(d_tr$T_Y), id = NA)
d_tr$Observer <- as.character(d_tr$Observer)
d_tr_all_obs <- left_join(d_tr_all, d_tr)
d_tr_all_obs <- d_tr_all_obs[ ,c(1,4)]
d_tr_all_obs <- d_tr_all_obs[which(!duplicated(d_tr_all_obs)), ] # Table with all sampled fields and which observer sampled it
sp <- d[which(d$Species == s_good[xxx]), which(colnames(d) %in% c("Year", "Banda", "transectID", "T_Y", "Species", "Observer", "Cluster"))] # Select species spAL and all years
sp <- arrange(sp, Year, transectID) #Ordered
sp_detec_transectID <- unique(sp$transectID)
sp$Observer <- as.character(sp$Observer)
absent <- anti_join(d_tr_all,sp) # Transects with 0 abundance, add to sp.
colnames(absent)[2] <- "Banda" # Format it to add the rows to sp
absent$T_Y <- as.character(absent$T_Y)
absent$Species <- s_good[xxx]
absent$Cluster <- NA
absent <- left_join(absent, d_tr_all_obs)
for (i in 1:nrow(absent)){ # Format to join absent - detections
cent <- substr(absent$T_Y[i], 10,10) # To include SI102 (cents)
cent <- as.numeric(cent)
if(is.na(cent)){
absent$Year[i] <- substr(absent$T_Y[i], 6,9)
absent$transectID[i] <- substr(absent$T_Y[i], 1,4)
} else { absent$Year[i] <- substr(absent$T_Y[i], 7,10)
absent$transectID[i] <- substr(absent$T_Y[i], 1,5)}
}
absent$count <- 0
sp$count <- 1
all_sp <- rbind(sp,absent) # Include transects with abundance 0
all_sp <- arrange(all_sp, Year, transectID) # Ordered
absent$count <- 0
yrs <- c(2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018) # I HAVE TO CONVERT THIS FROM 0-7 (but nyrs is still 8!)
nyrs <- length(yrs)
all.sites <- unique(all_sp$transectID)
all.sites <- sort(all.sites,descreasing = TRUE)
max.sites <- length(all.sites)
m <- matrix(NA, nrow = length(all.sites), ncol = nyrs)
rownames(m) <- all.sites
colnames(m) <- yrs
count <- aggregate(Species ~ Year + transectID, FUN = length, data = sp)
for (i in 1:nrow(count)){ # Add counts > 0
m[which(rownames(m) %in% count$transectID[i]), which(colnames(m) %in% count$Year[i])] <- count$Species[i]}
for (i in 1:nrow(absent)){ # Add absences (0)
m[which(rownames(m) %in% absent$transectID[i]), which(colnames(m) %in% absent$Year[i])] <- absent$count[i]}
at[[4]] <- colSums(m,na.rm = TRUE)
species[[xxx]] <- at
print(xxx)
}
save(species, file = "spConvergence_light.RData")
